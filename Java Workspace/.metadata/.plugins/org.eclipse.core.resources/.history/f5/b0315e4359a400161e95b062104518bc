//import home.laura.stanford-english-corenlp-2016-10-31-models;
import java.io.BufferedReader;
import java.io.File;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.IOException;
import java.io.StringReader;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.process.*;
public class StartPoint {

	public static void main(String[] args) {
		Map<String, Long> vocabulary = new HashMap<String, Long>();
		 try {
			BufferedReader bufferReader = new BufferedReader(new FileReader(""));
			String currentLine;
			File file = new File("/users/mkyong/filename.txt");
			// if file doesnt exists, then create it
			if (!file.exists()) {
				file.createNewFile();
			}
			
			while ((currentLine = bufferReader.readLine()) != null) {
				DocumentPreprocessor dp = new DocumentPreprocessor(currentLine);
			}
			
			
		} catch (Exception e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
		 
		 

		 String sent2 = "This is another sentence...";
		 TokenizerFactory<CoreLabel> tokenizerFactory =
				    PTBTokenizer.factory(new CoreLabelTokenFactory(), "");
	     List<CoreLabel> rawWords2 =
				    tokenizerFactory.getTokenizer(new StringReader(sent2)).tokenize();
		 System.out.println(rawWords2);		
	}
}
